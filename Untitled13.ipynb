{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install transformers bertviz tensorflow pandas matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "hNUvIQ7mhpYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrB8XD2p-m2"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from bertviz import head_view, model_view\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FifHgsHMqPQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "fake = pd.read_csv('/content/drive/MyDrive/Fake_news_english/Fake.csv')\n",
        "real = pd.read_csv('/content/drive/MyDrive/Fake_news_english/True.csv')\n",
        "\n",
        "# Explore the datasets\n",
        "print(\"Fake News Dataset Head:\")\n",
        "print(fake.head())\n",
        "\n",
        "print(\"\\nReal News Dataset Head:\")\n",
        "print(real.head())\n"
      ],
      "metadata": {
        "id": "MKLRvmKeqpO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize subject distribution in fake news\n",
        "plt.figure(figsize=(6,4))\n",
        "fake['subject'].value_counts().plot(kind='barh', color='red')\n",
        "plt.title('Distribution of Subjects in Fake News')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Subject')\n",
        "plt.show()\n",
        "\n",
        "# Visualize subject distribution in real news\n",
        "plt.figure(figsize=(6,4))\n",
        "real['subject'].value_counts().plot(kind='barh', color='green')\n",
        "plt.title('Distribution of Subjects in Real News')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Subject')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uhcXQweSqtOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label the datasets\n",
        "fake['label'] = 0  # Fake news labeled as 0\n",
        "real['label'] = 1  # Real news labeled as 1\n",
        "\n",
        "# Combine the datasets\n",
        "data = pd.concat([fake, real], ignore_index=True)\n",
        "print(\"Combined Dataset Shape:\", data.shape)\n"
      ],
      "metadata": {
        "id": "pgSesHkXq0It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns\n",
        "data = data.drop(columns=['subject', 'date'])\n",
        "\n",
        "# Combine title and text columns\n",
        "data['text'] = data['title'] + ' ' + data['text']\n",
        "\n",
        "# Keep only the necessary columns\n",
        "data = data[['text', 'label']]\n",
        "\n",
        "# Shuffle the data\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Processed Data Sample:\")\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "XylF7Kdxq45Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values in dataset:\", data.isnull().sum())\n"
      ],
      "metadata": {
        "id": "a9HklJSGq8Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "B3BZbcuNrCpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define maximum sequence length\n",
        "MAX_LEN = 256  # You can adjust this based on your computational resources"
      ],
      "metadata": {
        "id": "adaZzpfzryWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization function\n",
        "def tokenize_data(texts, labels):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf',\n",
        "            return_token_type_ids=False\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "    input_ids = tf.concat(input_ids, axis=0)\n",
        "    attention_masks = tf.concat(attention_masks, axis=0)\n",
        "    labels = tf.convert_to_tensor(labels)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n"
      ],
      "metadata": {
        "id": "vz3cvG2MkXbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data['text'],\n",
        "    data['label'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=data['label']\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Testing set size:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "PGnYkCBFkbLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize training data\n",
        "train_input_ids, train_attention_masks, train_labels = tokenize_data(X_train.tolist(), y_train.tolist())\n",
        "\n",
        "# Tokenize testing data\n",
        "test_input_ids, test_attention_masks, test_labels = tokenize_data(X_test.tolist(), y_test.tolist())\n"
      ],
      "metadata": {
        "id": "VCC9ImO4knta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': train_input_ids,\n",
        "        'attention_mask': train_attention_masks\n",
        "    },\n",
        "    train_labels\n",
        ")).shuffle(buffer_size=10000).batch(16)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': test_input_ids,\n",
        "        'attention_mask': test_attention_masks\n",
        "    },\n",
        "    test_labels\n",
        ")).batch(16)\n"
      ],
      "metadata": {
        "id": "rHnGsuyAmg79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertModel\n",
        "\n",
        "# Define the input layers\n",
        "input_ids = tf.keras.Input(shape=(256,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.Input(shape=(256,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create a custom layer to encapsulate the BERT model call\n",
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_model):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask = inputs\n",
        "        return self.bert_model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Create an instance of the custom layer\n",
        "bert_layer = BertLayer(bert_model)\n",
        "\n",
        "# Get the outputs from the BERT layer\n",
        "bert_output = bert_layer([input_ids, attention_mask])\n",
        "\n",
        "# Take the pooled output and add a dense layer for classification\n",
        "pooled_output = bert_output.pooler_output\n",
        "\n",
        "# Add a dropout layer for regularization\n",
        "dropout_layer = tf.keras.layers.Dropout(0.3)(pooled_output)\n",
        "\n",
        "dense_layer = tf.keras.layers.Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=dense_layer)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              loss='binary_crossentropy',  # Use 'sparse_categorical_crossentropy' for multi-class\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KR7IELK9pAnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model using the standard Keras method\n",
        "model.save(\"/content/drive/MyDrive/Fake_news_english/tfbert\")\n",
        "\n",
        "# You can also save the weights separately if needed\n",
        "model.save_weights(\"/content/drive/MyDrive/Fake_news_english/tfbert_weights.h5\")"
      ],
      "metadata": {
        "id": "5HY7RXpgqrFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary module\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "EMw3vsp2qMnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with a small number of epochs and smaller dataset\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,  # Small validation subset\n",
        "    epochs=10,  # Fewer epochs\n",
        "    verbose=1,\n",
        "\n",
        "    steps_per_epoch=100\n",
        ")\n"
      ],
      "metadata": {
        "id": "3N64pdrwwnNy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}